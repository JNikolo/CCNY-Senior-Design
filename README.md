# CCNY-Senior-Design
This repository is for the Senior Design class Project. 

# Net Design 
## Team Members
- [Jair Ruiz](https://github.com/JNikolo) - Leader
- [Usman Abbas](https://github.com/uscod) - Systems Savvy
- [Evan Perez](https://github.com/evanperez444) - Tech-Smith
## Project Idea
### RAG Playground
This project focuses on building/finetuning a deep learning model to perform abstractive text summarization. Additionally, the project aims to create an interactive platform for hands-on exploration of Retrieval-Augmented Generation (RAG) systems. Users will be able to compare the performance of the standalone model with that of the RAG system in summarization tasks. The platform will feature a user-friendly interface for real-time experimentation with RAG, offering customization of parameters, enhanced visualizations, and detailed performance metrics to facilitate in-depth analysis and understanding.
![image](https://github.com/JNikolo/CCNY-Senior-Design/assets/125705821/6c4abf42-2fba-4084-a32c-974465161a3d)

**Key Features:**
1. Enhanced Visualization: Incorporate advanced visualization tools to provide users with an intuitive understanding of how RAG models performed. Visualize answer correctness histogram, document embedding similarity map.

2. Abstractive Summarization: Training a deep learning model to perform abstractive summarization of texts.

3. Customization Options: Provide users with the ability to customize various parameters of the RAG models, such as the retrieval strategy, generation settings, and input data sources. This flexibility enables users to tailor the platform to their specific use cases and preferences.

4. Performance Metrics: Include performance metrics and analytics to evaluate the effectiveness of the RAG models. Measure response times, accuracy, and relevance of the generated responses to provide users with valuable insights into the model's performance and capabilities.

5. Performance Comparison: Allowing users to compare the summarization performance of our model against the AI system.
     
---
## Project Proposal

- You can access to our project proposal [here](SD_Propject_Proposal.pdf)  
Note: For a summarized version check the slides of our final presentation [here](SD_Final_presentation.pdf)

---
## Ideas collection (Background)
The following is a collection of previous work that relates to our project idea.
| Ideas | Overview | Components/Techniques | Technology/Libraries/Tools | Notes |
|-------|---------|-------|---------|---------|
| [fastRAG](https://github.com/IntelLabs/fastRAG) | FastRAG is a research framework developed by Intel Labs, designed for creating efficient and optimized retrieval-augmented generative (RAG) pipelines. <details><summary>**Read more**</summary>It includes LLM backends like Intel Gaudi Accelerators, ONNX Runtime, and Llama-CPP for running RAG pipelines efficiently, along with RAG-efficient components like Colbert for token-based late interaction, Fusion-in-Decoder (FiD) for generative multi-document encoding and decoding, and REPLUG for improved multi-document decoding.</details> | <details><summary>**Explore the components**</summary> <ul> <li><details><summary><b>REPLUG (Retrieve and Plug):</b></summary><ul><li>Retrieval-augmented LM method.</li><li>Documents are retrieved and plugged into the input using ensembling.</li><li>Works with any LM without fine-tuning.</li><li>Enables processing a larger number of retrieved documents without limiting to the LM context window.</li></ul> </details></li><li><details><summary><b>ColBERT v2 with PLAID Engine:</b></summary><ul><li>Dense retriever encoding documents into representative vectors.</li><li>ColBERT v2 reduces index size using vector quantization.</li><li>PLAID Engine improves latency times for ColBERT-based indexes.</li></ul></details></li><li><details><summary><b>PLAID Requirements:</b></summary><ul><li>Specifies GPU requirements for PLAID Engine usage.</li></ul></details></li><li><details><summary><b>fastRAG running LLMs with Habana Gaudi (DL1) and Gaudi 2:</b></summary><ul><li>Support for running LLMs on Intel Habana Gaudi accelerators.</li><li>Instructions for configuring the invocation layer of PromptModel for Gaudi backend.</li></ul></details></li><li><details><summary><b>fastRAG running LLMs with ONNX-runtime:</b></summary><ul><li>Method for running quantized LLMs efficiently on CPUs using ONNX-runtime.</li><li>Includes instructions for quantizing the model and loading the quantized model.</li></ul></details></li><li><details><summary><b>fastRAG Running RAG Pipelines with LLMs on a Llama CPP backend:</b></summary><ul><li>Method for running LLMs effectively on CPUs using llama-cpp.</li><li>Installation instructions and loading the model using LlamaCPPInvocationLayer.</li></ul></li><li><details><summary><b>Optimized Embedding Models:</b></summary><ul><li>Introduces quantized int8 models for bi-encoder rankers and retrievers.</li><li>Emphasizes low latency and high throughput.</li><li>Instructions for optimization and usage with the optimum-intel framework.</li></ul></details></li><li><details><summary><b>Fusion-In-Decoder (FiD):</b></summary><ul><li>Transformer-based generative model based on the T5 architecture.</li><li>Used for answering questions given relevant information.</li><li>Provides implementation as an invocation layer for LLMs and a training script for fine-tuning FiD models.</li></ul></li></ul></details> | <details> <summary>**Explore key libraries**</summary><ul><li>farm-haystack</li><li>transformers</li><li>datasets</li><li>evaluate</li><li>pandas</li><li>nltk</li><li>tqdm</li><li>numba</li><li>openpyxl</li><li>numpy</li><li>protobuf</li><li>ujson</li><li>accelerate</li><li>fastapi</li><li>uvicorn</li><li>Pillow</li><li>beir</li><li>kilt</li><li>streamlit</li><li>st-annotated-text</li><li>matplotlib</li><li>streamlit_chat</li><li>colbert-ai</li><li>faiss-gpu</li><li>faiss-cpu</li><li>qdrant-haystack</li><li>spacy</li><li>pyvis</li><li>networkx</li><li>opencv-python-headless</li><li>intel-extension-for-transformers</li><li>neural_compressor</li><li>pytrec_eval</li><li>torch</li><li>onnx</li><li>onnxruntime</li><li>onnxruntime-extensions</li><li>sentence-transformers</li><li>intel-extension-for-pytorch</li><li>optimum</li><li>llama-cpp-python</li><li>flake8</li></ul></details> | Useful project to have an idea how to build different types of RAG pipelines. Most likely not to be too much referenced, since it does not align too much with our idea.
| [Haystack](https://github.com/deepset-ai/haystack) | The goal of the LLM orchestration framework Haystack by deepset.ai is to create LLM applications that are adaptable and ready for production. <details><summary>**Read more**</summary>It facilitates the creation of pipelines for data interaction by joining different parts, such as file converters, vector databases, and models. It leverages textual data, including structured and unstructured documents, for retrieval and processing, making it perfect for RAG, question answering, semantic search, and chatbots. Although providing flexibility and extensive NLP capabilities, system complexity, data quality reliance, and the requirement for integration with current technologies may present implementation issues.</details> | <details><summary>**Explore the components**</summary><ul><li><details><summary>**Components**</summary><ul><li><b>Generators:</b> Responsible for generating text responses, divided into chat and non-chat types based on conversational contexts.</li><li><b>Retrievers:</b> Select documents matching user queries from Document Stores.</li></ul></details></li><li> <details><summary>**Document Stores**</summary>An object storing documents in Haystack, serving as an interface to a storage database. Various components can interact with it to read or write documents.</details></li><li> <details><summary>**Data Classes**</summary><ul><li><b>Document class:</b> Contains information carried through the pipeline, such as text, metadata, tables, or binary data.</li><li><b>Answer class:</b> Holds generated answers, originating queries, and metadata.</li></ul></details></li><li> <details><summary>**Pipelines**</summary>Customizable systems created by combining components, document stores, and integrations. Highly flexible, allowing various flows, standalone components, loops, and connections. Pipelines can be saved in convenient formats for reuse or sharing.</details></li><li> <details><summary>**Optimization**</summary> <ul><li><details><summary><strong>What is Model-Based Evaluation</strong></summary><p>Model-based evaluation in Haystack utilizes a language model to assess the results of a Pipeline, typically without requiring labels for outputs. It's commonly used with Retrieval-Augmented Generative (RAG) Pipelines but can be applied to any Pipeline. Haystack currently supports end-to-end model-based evaluation of complete RAG Pipelines.</p></details></li><li> <details><summary><strong>Using LLMs for Evaluation</strong></summary><p>A common strategy involves using Language Models (LLMs) like OpenAI's GPT models as evaluator models, often referred to as golden models. GPT-4 is frequently used for this purpose. This method offers flexibility in defining evaluation metrics, such as faithfulness and context relevance, through well-crafted prompts.</p></details> </li><li><details><summary><strong>Using Small Cross-Encoder Models</strong></summary><p>In addition to LLMs, small cross-encoder models can be used for evaluation, calculating semantic answer similarity, for example. These models are faster and cheaper but less flexible in terms of evaluation aspects compared to LLMs.</p></details></li><li> <details><summary><strong>Model-Based Evaluation Pipelines</strong></summary><p>Model-based evaluation in Haystack can be performed independently by creating and running an evaluation Pipeline, or by adding an evaluator component to the end of a RAG Pipeline. The latter approach allows both RAG Pipeline execution and evaluation in a single pipeline.run() call.</p></details></li><li> <details><summary><strong>Evaluation Framework Integrations</strong></summary><p>Haystack integrates with evaluation frameworks like DeepEval, UpTrain, and Ragas, providing Evaluator components for each framework: RagasEvaluator, DeepEvalEvaluator, and UpTrainEvaluator.</p></details></li></ul></details></ul> </details> | <details><summary>**Explore key libraries**</summary> <ul><li>Hatchling (build system)</li><li>Pandas</li><li>Haystack-bm25</li><li>Tqdm</li><li>Tenacity</li><li>Lazy-imports</li><li>Openai</li><li>Jinja2</li><li>Posthog</li><li>Pyyaml</li><li>More-itertools</li><li>Networkx</li><li>Typing_extensions</li><li>Boilerpy3</li><li>Requests</li><li>Numpy</li><li>Python-dateutil</li><li>Pre-commit</li><li>Mypy</li><li>Pytest</li><li>Pytest-cov</li><li>Pytest-custom_exit_code</li><li>Pytest-asyncio</li><li>Pytest-rerunfailures</li><li>Responses</li><li>Tox</li><li>Coverage</li><li>Python-multipart</li><li>Psutil</li><li>Pylint</li><li>Ruff</li><li>Toml</li><li>Reno</li><li>Dulwich</li><li>Black</li><li>Transformers</li><li>Huggingface_hub</li><li>Spacy</li><li>Spacy-curated-transformers</li><li>En-core-web-trf</li><li>Pypdf</li><li>Markdown-it-py</li><li>Mdit_plain</li><li>Tika</li><li>Azure-ai-formrecognizer</li><li>Langdetect</li><li>Sentence-transformers</li><li>Openai-whisper</li><li>Chroma-haystack</li><li>Jsonref</li><li>Openapi3</li><li>Jsonschema</li><li>Opentelemetry-sdk</li><li>Ddtrace</li><li>Structlog</li><li>Isort</li><li>Pyproject-parser</li><li>Haystack-pydoc-tools</li></ul></details>| This project is interesting to look at because it expands in the creation of AI tools. This will give us an idea on how we can create a powerful interface for users to create their RAG pipelines since Haystack is a fully functional framework. |
| [RAGs](https://github.com/run-llama/rags) | RAGs is a Streamlit app inspired by OpenAI's GPTs, allowing users to create a RAG pipeline using natural language. <details><summary>**Read more**</summary>Users can describe their task and specify parameters such as the number of documents to retrieve. The app provides a config view where users can adjust parameters like top-k and summarization. Finally, users can query the RAG agent with their questions over the specified data source.</details> | <details><summary>**Explore the components**</summary> <ul><li><details><summary>Home Page</summary>This is the section where you build a RAG pipeline by instructing the "builder agent". Typically to setup a RAG pipeline you need the following components:<ul><li>Describe the dataset. Currently they support either a single local file or a web page</li><li>Describe the task. Concretely this description will be used to initialize the "system prompt" of the LLM powering the RAG pipeline.</li><li>Define the typical parameters for a RAG setup. See the below section for the list of parameters.</li></ul></details></li><li><details><summary>RAG Config</summary><ul><li>This section contains the RAG parameters, generated by the "builder agent" in the previous section. In this section, you have a UI showcasing the generated parameters and have full freedom to manually edit/change them as necessary.</li><li>Currently the set of parameters is as follows:</li><ul><li>System Prompt</li><li>Include Summarization: whether to also add a summarization tool (instead of only doing top-k retrieval.)</li><li>Top-K</li><li>Chunk Size</li><li>Embed Model</li><li>LLM</li></ul><li>If you manually change parameters, you can press the "Update Agent" button in order to update the agent.</li><li>If you don't see the `Update Agent` button, that's because you haven't created the agent yet. Please go to the previous "Home" page and complete the setup process.</li><li>We can always add more parameters to make this more "advanced" 🛠️, but thought this would be a good place to start.</li></ul></details></li><li><details><summary>Generated RAG Agent</summary><ul><li>Once your RAG agent is created, you have access to this page.</li><li>This is a standard chatbot interface where you can query the RAG agent and it will answer questions over your data.</li><li>It will be able to pick the right RAG tools (either top-k vector search or optionally summarization) in order to fulfill the query.</li></ul></details></li><li><details><summary>Supported LLMs and Embeddings</summary><ul><li><details><summary>Builder Agent</summary><ul><li>By default, the builder agent uses OpenAI, specified in the <code>core/builder_config.py</code> file.</li><li>Customization to any desired LLM is possible, with an example provided for Anthropic.</li><li>Note: GPT-4 variants are recommended for reliable results in agent construction.</li></ul></details></li><li><details><summary>Generated RAG Agent</summary><ul><li>Configuration can be set through natural language or manually for both the embedding model and LLM.</li><li>LLMs supported:<ul><li>OpenAI: ID format is "openai:<model_name>", e.g., "openai:gpt-4-1106-preview".</li><li>Anthropic: ID format is "anthropic:<model_name>", e.g., "anthropic:claude-2".</li><li>Replicate: ID format is "replicate:<model_name>".</li><li>HuggingFace: ID format is "local:<model_name>", e.g., "local:BAAI/bge-small-en".</li></ul></li><li>Embeddings: Supports text-embedding-ada-002 by default, also supports Hugging Face models. Hugging Face models can be used by prefixing "local", e.g., "local:BAAI/bge-small-en".</li></ul></li></ul></details></li></ul></details> | <details><summary>**Explore key libraries**</summary><ul><li>streamlit</li><li>streamlit-pills</li><li>llama-index</li><li>llama-hub</li><li>langchain</li><li>pypdf</li><li>clip</li><li>typing-inspect</li><li>typing_extensions</li><li>types-requests</li><li>black</li><li>isort</li><li>pytest-asyncio</li><li>ruff</li><li>mypy</li><li>referencing</li><li>jsonschema-specifications</li><li>poetry</li><li>poetry-core</li></ul> | This project is very interesting since it gives us an idea on how we could implement the creation of RAG pipelines using natural language, which is very cool. This can lead into a very robust project for the creation of RAG pipelines since we can reach to people with no-code abilities.|
| [RAGArch](https://github.com/AI-ANK/RAGArch) | RAGArch is a streamlit application that allows users to test and compare RAG pipelines by giving them the option to choose different parameters such as the LLM, embeddings, and vector store. | <details><summary>**Explore the components**</summary>The application not only allows users to play with different settings with their own documents in real time, but also provides the option to export the Python code that corresponds to the pipeline the users configures so that they can put it into another project. The demo utilizes streamlit and LlamaindexThe LLM's that are included in this demo are Gemini Pro, Cohere, GPT 3.5, and GPT 4. The embeddings included are: <ul><li>"BAAI/bge-small-en-v1.5"</li><li>"WhereIsAI/UAE-Large-V1"</li><li>"BAAI/bge-large-en-v1.5"</li><li>"khoa-klaytn/bge-small-en-v1.5-angle"</li><li>"BAAI/bge-base-en-v1.5"</li><li>"llmrails/ember-v1"</li><li>"jamesgpt1/sf_model_e5"</li><li>"thenlper/gte-large"</li><li>"infgrad/stella-base-en-v2"</li><li>"thenlper/gte-base"</li></lu> Finally for Vectors Stores there is Simple, Pinecone, and Qdrant</details> | <details><summary>**Explore key libraries**</summary><ul><li>google-generativeai</li><li>llama-index</li><li>openai</li><li>cohere</li><li>sentence_transformers</li><li>pypdf</li><li>transformers</li><li>tree_sitter</li><li>tree_sitter_languages</li><li>chromadb</li><li>qdrant-client</li><li>pinecone-client</li></ul></details> | This project aligns with the idea of creating the RAG pipelines using an interface where users don't need to code. This is useful to give us an idea what we should include and how we can do it. |

## Datasets
The following is a collection of datasets that can be used to perform summarization tasks.
| Datasets | Overview | Size | Structure | Domain | To be used | Notes | 
|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|
| [WikiHow-Dataset](https://github.com/mahnazkoupaee/WikiHow-Dataset/tree/master) | This is a large-scale summarization dataset consisting of diverse articles form WikiHow knowledge base. | +200K pairs of articles and summaries | The dataset is presented in a .csv file format and the columns are Title, Headline, and Text. <details><summary>**Read more**</summary><ul><li><b>Title:</b> The title of the article as it appears on the WikiHow knowledge base</li><li><b>Headline:</b> The concatenation of all the bold lines (the summary sentences) of all the paragraphs to serve as the reference summary</li><li><b>Text:</b> The concatenation of all paragraphs (except the bold lines) to generate the article to be summarized</li></ul></details> | Knowledge Base | ✅ | The dataset needs to be processed using the script provided in the source repo. It will download all the summaries and articles for each of the titles. |
| [Scientific Papers Dataset](https://huggingface.co/datasets/armanc/scientific_papers)  | This large dataset contains two sets of long and structured documents. The datasets are obtained from ArXiv and PubMed OpenAccess repositories. | The ArXiv and PubMed datasets have +200K and +100k rows respectively. <details><summary>**Read more**</summary><ul><li><b>arXiv:</b><ul><li><b>Train:</b> 203,037</li><li><b>Validation:</b> 6,436</li><li><b>Test:</b> 6,440</li></ul></li><li><b>PubMed:</b><ul><li><b>Train:</b> 119,924</li><li><b>Validation:</b> 6,633</li><li><b>Test:</b> 6,658</li></ul></li></ul> | Both datasets have the columns: article, abstract, section_names; all the features are string.<details><summary>**Read more**</summary><ul><li><b>Article:</b> The body of the document, paragraphs separated by "/n".</li><li><b>Abstract:</b> The abstract of the document, paragraphs separated by "/n".</li><li><b>Section Names:</b> Titles of sections, separated by "/n".</li></ul></details> | Academic Paper | ✅ | In these datasets, the abstract is used as the summary. | 
| [CNN/Daily Mail Dataset](https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail) | The CNN / DailyMail Dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. | This dataset contains +300k rows. <details><summary>**Read more**</summary><ul><li><b>Dataset Split:</b><ul><li><b>Train:</b> 287,113 instances</li><li><b>Validation:</b> 13,368 instances</li><li><b>Test:</b> 11,490 instances</li></ul></li></ul></details> | The dataset contains the columns: id, article and highlights. All of them are strings.<details><summary>**Read more**</summary><ul><li><b>ID:</b> A string containing the hexadecimal formatted SHA1 hash of the URL where the story was retrieved from</li><li><b>Article:</b> A string containing the body of the news article</li><li><b>Highlights:</b> A string containing the highlight of the article as written by the article author</li></ul></details> | News | ✅ | In the dataset, the highlights field is used as the summary of the text to perform evaluation. |

## Techniques/methods
The following is a collection of techniques for text summarization and to create the RAG systems.
### Text Summarization
| Technique | Overview | Key Features | Pros/Cons | Use-cases | when is needed? | Notes |
|-----------|----------|--------------|-----------|-----------|------------|-------|
| [Graph-enhanced Multi-Document Summarization (MDS)](https://aclanthology.org/2021.naacl-main.380.pdf) | An efficient graph-enhanced approach to multi-document summarization using an encoder-decoder Transformer model. | <details><summary>**Explore key features**</summary><ul><li>Incorporates an efficient encoding mechanism that avoids quadratic memory growth.</li> <li>Utilizes graph representations derived from multi-document clusters.</li> <li>Pre-trained on very large text data.</li></ul></details>|<details><summary>**Explore pros/cons**</summary> Pros: Scales to large input documents, improves summary abstraction, and is more informative and factually consistent. Cons: Requires additional processing for graph representations. </details>| <details><summary>**Explore use**</summary><ul><li>Summarizing news clusters.</li><li>Any task involving summarization of large multi-document datasets.</li></ul></details> | When dealing with large multi-document clusters and when factual consistency and informativeness are critical. | The approach leads to significant improvements on the Multi-News dataset and shows transfer improvements on the DUC-2004 dataset. Provides a 1.8 ROUGE score improvement over previous work. |
| [Query-Focused Text Summarization (QFTS)](https://direct.mit.edu/coli/article/48/2/279/109901/Domain-Adaptation-with-Pre-trained-Transformers) | Generates summaries of text documents based on a given query using transformer models. | <details><summary>**Explore key features**</summary> <ul><li>Utilizes pre-trained transformer models.</li> <li>Applies domain adaptation techniques including transfer learning, weakly supervised learning, and distant supervision.</li></ul></details> |<details><summary>**Explore pros/cons**</summary> Pros: Effective in generating abstractive summaries, sets new state-of-the-art results, versatile across single and multi-document scenarios. Cons: Lack of large labeled data for training, complexity in implementing domain adaptation techniques. </details> |<details><summary>**Explore use**</summary> <ul><li>Query-focused summarization for both single and multi-document scenarios.</li> <li>Tasks requiring summaries tailored to specific queries.</li> </ul></details> | When the goal is to generate summaries relevant to specific queries, especially in domains with limited labeled data. | Extensive experiments on six datasets demonstrate the effectiveness of the approach, achieving new state-of-the-art results across various evaluation metrics. |
| [Hierarchical Propagation Layer for Transformer Models](https://aclanthology.org/2021.eacl-main.154.pdf) | A novel layer designed to enhance transformer-based architectures for reasoning with long documents. | <details><summary>**Explore key features**</summary><ul><li>Divides input into multiple blocks.</li><li>Independently processes each block with scaled dot-attentions.</li><li>Combines information between successive layers.</li></ul></details> | <details><summary>**Explore pros/cons**</summary> Pros:<ul><li>Effective for long document summarization.</li><li>Achieves state-of-the-art results.</li></ul> Cons:<ul><li>Complexity in implementation.</li><li>Potentially higher computational resources needed.</li></ul> </details> | <details><summary> **Explore use** </summary><ul><li>Extractive summarization of long scientific papers.</li><li>Extractive summarization of long news articles.</li></ul></details> | When dealing with tasks that require understanding and summarizing long documents, especially in research and media domains. | This technique provides a hierarchical approach that improves over standard transformers, particularly useful for tasks requiring processing of extensive text. Validated on three corpora, demonstrating superior performance for long documents and competitive results for shorter ones. |
| [Hierarchical Latent Structure for Transformer Models](https://arxiv.org/pdf/2203.07586) | A framework designed to improve text summarization by addressing long-range dependencies and efficiency issues in transformer models. | <details><summary>**Explore key features**</summary><ul><li>Assumes a hierarchical latent structure in documents.</li><li>Top-level captures long-range dependencies at a coarser time scale.</li><li>Bottom token level preserves details.</li><li>Combines bottom-up and top-down inference.</li><li>Efficient local self-attention for bottom-up pass.</li><li>Top-down correction for capturing long-range dependencies.</li></ul></details> | <details><summary>**Explore pros/cons**</summary> <b>Pros:</b><ul><li>Improved performance on long document summarization.</li><li>Competitive performance on short documents with higher memory and compute efficiency.</li><li>Can summarize entire books with significantly fewer parameters and training data.</li></ul> <b>Cons:</b><ul><li>Potential complexity in implementing hierarchical structure and inference passes.</li></ul> </details> | <details><summary>**Explore use**</summary><ul><li>Summarization of narrative documents.</li><li>Summarization of conversational documents.</li><li>Summarization of scientific documents.</li><li>Summarization of news articles.</li><li>Summarization of entire books.</li></ul></details> | When high efficiency and ability to capture long-range dependencies are crucial, especially for diverse and extensive text summarization tasks. | Demonstrates state-of-the-art performance on long documents and competitive results on short documents, using significantly fewer parameters and training data compared to GPT-3-based models. Applicable to a wide range of document types. |
| [Topic Assistant (TA) for Transformer Models](https://aclanthology.org/2020.emnlp-main.35.pdf) | A plug-and-play module designed to integrate topic models with Transformer-based models to improve abstractive document summarization. | <details><summary>**Explore key features**</summary><ul><li>Rearranges and explores semantics learned by a topic model.</li><li>Compatible with various Transformer-based models.</li><li>Does not alter the original Transformer structure.</li><li>Introduces a small number of extra parameters.</li></ul></details> | <details><summary>**Explore pros/cons**</summary> <b>Pros:</b><ul><li>Enhances document understanding and summary generation.</li><li>Easy to fine-tune with pre-trained models.</li><li>Minimal additional parameters required.</li></ul> <b>Cons:</b><ul><li>Effectiveness may vary depending on the specific Transformer model and task.</li></ul> </details> | <details><summary>**Explore use**</summary><ul><li>Abstractive summarization of diverse document types.</li><li>Applications in any domain requiring enhanced summarization capabilities.</li></ul></details> | When there is a need to boost the performance of Transformer-based summarizers with better document semantics understanding. | Experimental results show improved performance on several datasets, indicating the general applicability and effectiveness of TA in enhancing Transformer-based summarization models. |
| [Socratic Pretraining](https://arxiv.org/pdf/2212.10449) | A question-driven, unsupervised pretraining objective designed to improve controllability in summarization tasks. | <details><summary>**Explore key features**</summary><ul><li>Trains models to generate and answer questions relevant to the context.</li><li>Enhances adherence to user queries and identification of relevant content.</li><li>Relies only on unlabeled documents and a question generation system.</li></ul></details> | <details><summary>**Explore pros/cons**</summary> <b>Pros:</b><ul><li>Reduces the need for labeled data by half.</li><li>More faithful to user-provided queries.</li><li>Achieves state-of-the-art performance.</li></ul> <b>Cons:</b><ul><li>Effectiveness may depend on the quality of the question generation system.</li></ul> </details> | <details><summary>**Explore use**</summary><ul><li>Controllable summarization of long documents.</li><li>Domains such as short stories and dialogue.</li></ul></details> | When labeled data is scarce and there is a need for high controllability in summarization. | Demonstrated to outperform pre-finetuning approaches that use additional supervised data. Validated through extensive experimentation on multiple control strategies, showing state-of-the-art performance on QMSum and SQuALITY. |
| [Hybrid Pointer-Generator Network with Coverage](https://arxiv.org/pdf/1704.04368) | A novel architecture for abstractive text summarization that addresses inaccuracies and repetition in neural sequence-to-sequence models. | <details><summary>**Explore key features**</summary><ul><li>Combines standard sequence-to-sequence attentional model with two orthogonal augmentations.</li><li>Pointer-generator network allows copying words from the source text and generating novel words.</li><li>Coverage mechanism tracks summarized content to discourage repetition.</li></ul></details> | <details><summary>**Explore pros/cons**</summary> <b>Pros:</b><ul><li>Improves accuracy of factual details.</li><li>Reduces repetition in summaries.</li><li>Outperforms current state-of-the-art models.</li></ul> <b>Cons:</b><ul><li>Potential complexity in implementation.</li></ul> </details> | <details><summary>**Explore use**</summary><ul><li>Abstractive summarization tasks, particularly in news summarization (e.g., CNN / Daily Mail).</li></ul></details> | When accuracy and non-repetition are critical in abstractive summarization. | Demonstrated to achieve state-of-the-art performance, outperforming previous models by at least 2 ROUGE points on the CNN / Daily Mail summarization task. |
| [Hybrid Extractive-Abstractive Model with BERT and Reinforcement Learning](https://www.mdpi.com/2076-3417/9/21/4701) | A hybrid model combining extractive and abstractive summarization using BERT word embeddings and reinforcement learning. | <details><summary>**Explore key features**</summary><ul><li>Converts human-written abstractive summaries to ground truth labels.</li><li>Uses BERT for text representation.</li><li>Pre-trains two sub-models: extraction and abstraction networks.</li><li>Bridges extraction and abstraction networks with reinforcement learning.</li></ul></details> | <details><summary>**Explore pros/cons**</summary> <b>Pros:</b><ul><li>Combines strengths of both extractive and abstractive summarization.</li><li>Improves accuracy significantly.</li><li>Uses advanced techniques like BERT and reinforcement learning.</li></ul> <b>Cons:</b><ul><li>Potentially high computational complexity.</li></ul> </details> | <details><summary>**Explore use**</summary><ul><li>Automatic text summarization in various fields.</li><li>Particularly useful for datasets like CNN/Daily Mail.</li></ul></details> | When high accuracy in summarization is crucial, and there is access to computational resources for implementing complex models. | Extensively tested on the CNN/Daily Mail dataset, showing improved accuracy as measured by ROUGE metrics. Demonstrates the effectiveness of combining extractive and abstractive methods with reinforcement learning. |
| [Generative Model with Convolutional Seq2Seq Architecture](https://www.mdpi.com/2076-3417/9/8/1665) | A generative model for abstractive text summarization based on a convolutional seq2seq architecture. | <details><summary>**Explore key features**</summary><ul><li>Hierarchical CNN framework for efficiency.</li><li>Copying mechanism for handling rare or unseen words.</li><li>Hierarchical attention mechanism for modeling keywords and key sentences.</li></ul></details> | <details><summary>**Explore pros/cons**</summary> <b>Pros:</b><ul><li>More efficient than conventional RNN seq2seq models.</li><li>Effectively handles rare or unseen words.</li><li>Outperforms state-of-the-art models significantly.</li></ul> <b>Cons:</b><ul><li>May have complexity in incorporating hierarchical attention and copying mechanisms.</li></ul> </details> | <details><summary>**Explore use**</summary><ul><li>Abstractive summarization tasks.</li><li>Tested on datasets like GigaWord and DUC corpus.</li></ul></details> | When there is a need for efficient and accurate abstractive summarization. | Experiment results show the model consistently outperforms state-of-the-art alternatives on real-life datasets. Incorporates advanced mechanisms to enhance summarization quality. |
| [Attentional Encoder-Decoder RNNs for Abstractive Summarization](https://arxiv.org/pdf/1602.06023) | Models abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, achieving state-of-the-art performance on different corpora. | <details><summary>**Explore key features**</summary><ul><li>Several novel models addressing critical summarization challenges.</li><li>Models key-words, hierarchy of sentence-to-word structure, and rare/unseen words.</li><li>Proposed dataset with multi-sentence summaries and performance benchmarks.</li></ul></details> | <details><summary>**Explore pros/cons**</summary> <b>Pros:</b><ul><li>Achieves state-of-the-art performance.</li><li>Addresses critical summarization challenges.</li><li>Establishes benchmarks for further research.</li></ul> <b>Cons:</b><ul><li>May require significant computational resources.</li><li>Potential complexity in implementing novel models.</li></ul> </details> | <details><summary>**Explore use**</summary><ul><li>Abstractive summarization tasks across different domains.</li><li>Research requiring performance benchmarks in abstractive summarization.</li></ul></details> | When seeking state-of-the-art performance and addressing critical challenges in abstractive text summarization. | Proposed models contribute to further improvement in summarization performance and establish benchmarks for future research. |
| [Topic-Enhanced ConvS2S Model with Self-Critical Sequence Training](https://arxiv.org/pdf/1805.03616) | A deep learning approach for automatic summarization tasks, integrating topic information into the Convolutional Sequence-to-Sequence (ConvS2S) model and utilizing Self-Critical Sequence Training (SCST) for optimization. | <details><summary>**Explore key features**</summary><ul><li>Incorporates topic information into the ConvS2S model.</li><li>Uses SCST for optimization, improving coherence, diversity, and informativeness of generated summaries.</li><li>Biased probability generation mechanism improves summary quality.</li></ul></details> | <details><summary>**Explore pros/cons**</summary> <b>Pros:</b><ul><li>Improves coherence, diversity, and informativeness of summaries.</li><li>Avoids exposure bias during inference.</li><li>Demonstrates superiority over state-of-the-art methods in abstractive summarization.</li></ul> <b>Cons:</b><ul><li>May require significant computational resources for training.</li><li>Complexity in implementing SCST and biased probability generation mechanism.</li></ul> </details> | <details><summary>**Explore use**</summary><ul><li>Abstractive summarization tasks across various datasets.</li></ul></details> | When aiming to enhance summary quality through topic integration and optimization with SCST. | Empirical results show the superiority of the proposed method over state-of-the-art approaches in abstractive summarization, validated on multiple datasets including Gigaword, DUC-2004, and LCSTS. |


### RAG system development
| Technique | Overview | Pros/Cons | Use-cases | Tools/Software | To be used | Notes |
|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-------------|
|[Building an AI Chatbot with RAG, Langchain, and Streamlit](https://www.linkedin.com/pulse/building-ai-chatbot-rag-langchain-streamlit-sachin-samuel-n9tef/)| This technique presents a way to integrate a robust LLM framework like Langchain, and a friendly front-end framework like Streamlit to showcase RAG capabilities. | <details><summary>**Explore pros/cons**</summary> **Pros:** <ul><li>Integration of Robust Framework: The technique integrates Langchain, a robust Large Language Model (LLM) framework, providing powerful language processing capabilities.</li><li>User-Friendly Front-End: Streamlit, a friendly front-end framework, is used to create an intuitive and interactive interface for showcasing RAG capabilities.</li></ul> **Cons:** <ul><li>Technical Complexity: Implementing an AI chatbot with RAG, Langchain, and Streamlit may require a certain level of technical expertise, especially in setting up and configuring the components.</li><li>Resource Intensive: Running an AI chatbot with these frameworks may be resource-intensive, requiring sufficient computational resources and infrastructure.</li></ul> <ul></details> |<details><summary>**Explore use-cases**</summary> <li>Customer Service: The AI chatbot can be deployed for customer service applications, providing automated responses to customer inquiries and support requests.</li><li>Information Retrieval: Users can interact with the chatbot to retrieve information on various topics, leveraging the language processing capabilities of Langchain and the interactive interface of Streamlit.</li><li>Educational Tools: The chatbot can serve as an educational tool, assisting users with learning and understanding concepts in natural language processing and AI.</li></ul></details> | <ul><li>Streamlit</li><li>Langchain</li></ul> | ✅ | Optimum technique to showcase our project to the world. Streamlit is easy to use, and langchain a very powerful framework. |
| [Abstractive Long Text Summarization using Large Language Models](https://ijisae.org/index.php/IJISAE/article/view/4500/3160) | A novel approach for addressing the challenge of retaining context over extensive texts or multiple documents using Large Language Models (LLMs). The methodology focuses on improving summarization and question answering tasks by preventing LLM overload with unrelated, repetitive, or redundant data. | <details><summary>**Explore pros/cons**</summary> **Pros:** <ul><li>Enhances summarization and question answering by retaining relevant context.</li><li>Saves time and resources by avoiding processing of unrelated or redundant data.</li><li>Improves overall performance and efficiency of LLMs.</li></ul> **Cons:** <ul><li>Implementation complexity may vary based on LLM architecture and task requirements.</li></ul></details> | <details><summary>**Explore use-cases**</summary> - Large-scale document summarization. <br> - Question answering systems for extensive texts or multiple documents.</details> | - Large Language Models (LLMs). <br> - Summarization and Question Answering frameworks. | ✅ | The proposed approach aims to optimize LLMs for better context retention, leading to more effective summaries and answers, thus enhancing overall system performance and efficiency. |
| [Retrieval Augmented Generation (RAG)-based Summarization AI for EIC](https://arxiv.org/pdf/2403.15729) | A two-step approach involving a Retrieval Augmented Generation (RAG)-based Summarization AI for Electron Ion Collider (EIC) community. The AI-Agent condenses information and references relevant responses, offering substantial advantages for collaborators. | <details><summary>**Explore pros/cons**</summary> **Pros:** <ul><li>Condenses vast and complex information into concise summaries.</li><li>Effectively references relevant responses, enhancing collaboration.</li><li>Utilizes RAG assessments (RAGAs) scoring mechanisms for evaluation.</li></ul> **Cons:** <ul><li>Complexity in development and integration.</li><li>Potential challenges in ensuring accuracy and relevance of generated summaries.</li></ul></details> | <details><summary>**Explore use-cases**</summary> - Accessing and utilizing large-scale experiment information. <br> - Collaborative research in the Electron Ion Collider (EIC) community.</details> | - Large Language Models (LLMs). <br> - LangChain for workflow foundation. <br> - Web application for demonstration. | ✅ | Integrates retrieval augmented generation techniques with Large Language Models (LLMs) to summarize and reference relevant responses, facilitating access to complex experiment information and promoting collaborative research. |
| [Graph RAG Approach for Question Answering](https://arxiv.org/pdf/2404.16130) | A method combining retrieval-augmented generation (RAG) and query-focused summarization (QFS) for question answering over private text corpora. Utilizes large language models (LLMs) to build a graph-based text index and pre-generate community summaries for closely-related entities, leading to improved scalability and performance compared to traditional RAG and QFS methods. | <details><summary>**Explore pros/cons**</summary> **Pros:** <ul><li>Scales with the generality of user questions and the quantity of source text.</li><li>Improves comprehensiveness and diversity of generated answers.</li><li>Provides a solution for global sensemaking questions over large datasets.</li></ul> **Cons:** <ul><li>Complexity in implementation and integration.</li><li>Requires significant computational resources for large-scale text indexing.</li></ul></details> | <details><summary>**Explore use-cases**</summary> - Question answering over private text corpora. <br> - Sensemaking tasks over large datasets.</details> | - Large Language Models (LLMs). <br> - Python-based implementation forthcoming at [Graph RAG GitHub](https://github.com/GraphRAG). | ✅ | Combines RAG and QFS methods to improve question answering over private text corpora. Provides scalability and performance improvements over traditional approaches, demonstrated through substantial improvements in answer comprehensiveness and diversity. |


## More About Us
[Know about our team](https://docs.google.com/presentation/d/1SBlGVdz81NUZDpsXQ5xZXaC7oOi-OAkKURFXmy4CcT8/edit?usp=sharing)

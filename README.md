# CCNY-Senior-Design
This repository is for the Senior Design class Project. 

# Net Design 
## Team Members
- [Jair Ruiz](https://github.com/JNikolo) - Leader
- [Usman Abbas](https://github.com/uscod) - Systems Savvy
- [Evan Perez](https://github.com/evanperez444) - Tech-Smith
## Project Idea
### RAG Playground
This project aims to create an interactive platform designed for hands-on exploration of Retrieval-Augmented Generation (RAG) models. It offers users a user-friendly interface to experiment with RAG in real-time, allowing for parameter customization, enhanced visualization, integration with external APIs, performance metrics, and accessibility features. Also, it will be integrating a fine-tuned LLM that can classify the RAG pipelines on different classes ("Good", "poor", "excellent", "bad", "Okay") to give the user a better understanding on what he needs to improve.
![image](https://github.com/JNikolo/CCNY-Senior-Design/assets/125705821/3606ff26-e0e6-44bb-8a69-9a7d40e46d67)



**Key Features:**
1. Enhanced Visualization: Incorporate advanced visualization tools to provide users with an intuitive understanding of how RAG models performed. Visualize answer correctness histogram, document embedding similarity map.

2. Integration with External Data: Allow integration with external datasets to expand the capabilities of the application. Users can access specialized databases or domain-specific knowledge sources to enhance the retrieval process and generate more accurate responses.

3. Customization Options: Provide users with the ability to customize various parameters of the RAG models, such as the retrieval strategy, generation settings, and input data sources. This flexibility enables users to tailor the platform to their specific use cases and preferences.

4. Performance Metrics: Include performance metrics and analytics to evaluate the effectiveness of the RAG models. Measure response times, accuracy, and relevance of the generated responses to provide users with valuable insights into the model's performance and capabilities.

5. Improving RAG: Helps users with recommendations based on the performance metrics results using a fine-tuned LLM. The user tries again and the previous results are shown with the current ones.
     
---

## Ideas collection (Background)

| Ideas | Overview | Components/Techniques | Technology/Libraries/Tools | Notes |
|-------|---------|-------|---------|---------|
| [fastRAG](https://github.com/IntelLabs/fastRAG) | FastRAG is a research framework developed by Intel Labs, designed for creating efficient and optimized retrieval-augmented generative (RAG) pipelines. It includes LLM backends like Intel Gaudi Accelerators, ONNX Runtime, and Llama-CPP for running RAG pipelines efficiently, along with RAG-efficient components like Colbert for token-based late interaction, Fusion-in-Decoder (FiD) for generative multi-document encoding and decoding, and REPLUG for improved multi-document decoding. | <details><summary>**See the components**</summary> <ul> <li><details><summary><b>REPLUG (Retrieve and Plug):</b></summary><ul><li>Retrieval-augmented LM method.</li><li>Documents are retrieved and plugged into the input using ensembling.</li><li>Works with any LM without fine-tuning.</li><li>Enables processing a larger number of retrieved documents without limiting to the LM context window.</li></ul> </details></li><li><details><summary><b>ColBERT v2 with PLAID Engine:</b></summary><ul><li>Dense retriever encoding documents into representative vectors.</li><li>ColBERT v2 reduces index size using vector quantization.</li><li>PLAID Engine improves latency times for ColBERT-based indexes.</li></ul></details></li><li><details><summary><b>PLAID Requirements:</b></summary><ul><li>Specifies GPU requirements for PLAID Engine usage.</li></ul></details></li><li><details><summary><b>fastRAG running LLMs with Habana Gaudi (DL1) and Gaudi 2:</b></summary><ul><li>Support for running LLMs on Intel Habana Gaudi accelerators.</li><li>Instructions for configuring the invocation layer of PromptModel for Gaudi backend.</li></ul></details></li><li><details><summary><b>fastRAG running LLMs with ONNX-runtime:</b></summary><ul><li>Method for running quantized LLMs efficiently on CPUs using ONNX-runtime.</li><li>Includes instructions for quantizing the model and loading the quantized model.</li></ul></details></li><li><details><summary><b>fastRAG Running RAG Pipelines with LLMs on a Llama CPP backend:</b></summary><ul><li>Method for running LLMs effectively on CPUs using llama-cpp.</li><li>Installation instructions and loading the model using LlamaCPPInvocationLayer.</li></ul></li><li><details><summary><b>Optimized Embedding Models:</b></summary><ul><li>Introduces quantized int8 models for bi-encoder rankers and retrievers.</li><li>Emphasizes low latency and high throughput.</li><li>Instructions for optimization and usage with the optimum-intel framework.</li></ul></details></li><li><details><summary><b>Fusion-In-Decoder (FiD):</b></summary><ul><li>Transformer-based generative model based on the T5 architecture.</li><li>Used for answering questions given relevant information.</li><li>Provides implementation as an invocation layer for LLMs and a training script for fine-tuning FiD models.</li></ul></li></ul></details> | <details> <summary>**See the used libraries**</summary><ul><li>farm-haystack</li><li>transformers</li><li>datasets</li><li>evaluate</li><li>pandas</li><li>nltk</li><li>tqdm</li><li>numba</li><li>openpyxl</li><li>numpy</li><li>protobuf</li><li>ujson</li><li>accelerate</li><li>fastapi</li><li>uvicorn</li><li>Pillow</li><li>beir</li><li>kilt</li><li>streamlit</li><li>st-annotated-text</li><li>matplotlib</li><li>streamlit_chat</li><li>colbert-ai</li><li>faiss-gpu</li><li>faiss-cpu</li><li>qdrant-haystack</li><li>spacy</li><li>pyvis</li><li>networkx</li><li>opencv-python-headless</li><li>intel-extension-for-transformers</li><li>neural_compressor</li><li>pytrec_eval</li><li>torch</li><li>onnx</li><li>onnxruntime</li><li>onnxruntime-extensions</li><li>sentence-transformers</li><li>intel-extension-for-pytorch</li><li>optimum</li><li>llama-cpp-python</li><li>flake8</li></ul></details> | Useful project to have an idea how to build different types of RAG pipelines. Most likely not to be too much referenced, since it does not align too much with our idea.
| [Haystack](https://github.com/deepset-ai/haystack) | The goal of the LLM orchestration framework Haystack by deepset.ai is to create LLM applications that are adaptable and ready for production. It facilitates the creation of pipelines for data interaction by joining different parts, such as file converters, vector databases, and models. It leverages textual data, including structured and unstructured documents, for retrieval and processing, making it perfect for RAG, question answering, semantic search, and chatbots. Although providing flexibility and extensive NLP capabilities, system complexity, data quality reliance, and the requirement for integration with current technologies may present implementation issues. | <details><summary>**See Techniques/Components**</summary><ul><li><details><summary>**Components**</summary><ul><li><b>Generators:</b> Responsible for generating text responses, divided into chat and non-chat types based on conversational contexts.</li><li><b>Retrievers:</b> Select documents matching user queries from Document Stores.</li></ul></details></li><li> <details><summary>**Document Stores**</summary>An object storing documents in Haystack, serving as an interface to a storage database. Various components can interact with it to read or write documents.</details></li><li> <details><summary>**Data Classes**</summary><ul><li><b>Document class:</b> Contains information carried through the pipeline, such as text, metadata, tables, or binary data.</li><li><b>Answer class:</b> Holds generated answers, originating queries, and metadata.</li></ul></details></li><li> <details><summary>**Pipelines**</summary>Customizable systems created by combining components, document stores, and integrations. Highly flexible, allowing various flows, standalone components, loops, and connections. Pipelines can be saved in convenient formats for reuse or sharing.</details></li><li> <details><summary>**Optimization**</summary> <ul><li><details><summary><strong>What is Model-Based Evaluation</strong></summary><p>Model-based evaluation in Haystack utilizes a language model to assess the results of a Pipeline, typically without requiring labels for outputs. It's commonly used with Retrieval-Augmented Generative (RAG) Pipelines but can be applied to any Pipeline. Haystack currently supports end-to-end model-based evaluation of complete RAG Pipelines.</p></details></li><li> <details><summary><strong>Using LLMs for Evaluation</strong></summary><p>A common strategy involves using Language Models (LLMs) like OpenAI's GPT models as evaluator models, often referred to as golden models. GPT-4 is frequently used for this purpose. This method offers flexibility in defining evaluation metrics, such as faithfulness and context relevance, through well-crafted prompts.</p></details> </li><li><details><summary><strong>Using Small Cross-Encoder Models</strong></summary><p>In addition to LLMs, small cross-encoder models can be used for evaluation, calculating semantic answer similarity, for example. These models are faster and cheaper but less flexible in terms of evaluation aspects compared to LLMs.</p></details></li><li> <details><summary><strong>Model-Based Evaluation Pipelines</strong></summary><p>Model-based evaluation in Haystack can be performed independently by creating and running an evaluation Pipeline, or by adding an evaluator component to the end of a RAG Pipeline. The latter approach allows both RAG Pipeline execution and evaluation in a single pipeline.run() call.</p></details></li><li> <details><summary><strong>Evaluation Framework Integrations</strong></summary><p>Haystack integrates with evaluation frameworks like DeepEval, UpTrain, and Ragas, providing Evaluator components for each framework: RagasEvaluator, DeepEvalEvaluator, and UpTrainEvaluator.</p></details></li></ul></details></ul> </details> | <details><summary>**See Libraries**</summary> <ul><li>Hatchling (build system)</li><li>Pandas</li><li>Haystack-bm25</li><li>Tqdm</li><li>Tenacity</li><li>Lazy-imports</li><li>Openai</li><li>Jinja2</li><li>Posthog</li><li>Pyyaml</li><li>More-itertools</li><li>Networkx</li><li>Typing_extensions</li><li>Boilerpy3</li><li>Requests</li><li>Numpy</li><li>Python-dateutil</li><li>Pre-commit</li><li>Mypy</li><li>Pytest</li><li>Pytest-cov</li><li>Pytest-custom_exit_code</li><li>Pytest-asyncio</li><li>Pytest-rerunfailures</li><li>Responses</li><li>Tox</li><li>Coverage</li><li>Python-multipart</li><li>Psutil</li><li>Pylint</li><li>Ruff</li><li>Toml</li><li>Reno</li><li>Dulwich</li><li>Black</li><li>Transformers</li><li>Huggingface_hub</li><li>Spacy</li><li>Spacy-curated-transformers</li><li>En-core-web-trf</li><li>Pypdf</li><li>Markdown-it-py</li><li>Mdit_plain</li><li>Tika</li><li>Azure-ai-formrecognizer</li><li>Langdetect</li><li>Sentence-transformers</li><li>Openai-whisper</li><li>Chroma-haystack</li><li>Jsonref</li><li>Openapi3</li><li>Jsonschema</li><li>Opentelemetry-sdk</li><li>Ddtrace</li><li>Structlog</li><li>Isort</li><li>Pyproject-parser</li><li>Haystack-pydoc-tools</li></ul></details>| This project is interesting to look at because it expands in the creation of AI tools. This will give us an idea on how we can create a powerful interface for users to create their RAG pipelines since Haystack is a fully functional framework. |
| [RAGs](https://github.com/run-llama/rags) | RAGs is a Streamlit app inspired by OpenAI's GPTs, allowing users to create a RAG pipeline using natural language. Users can describe their task and specify parameters such as the number of documents to retrieve. The app provides a config view where users can adjust parameters like top-k and summarization. Finally, users can query the RAG agent with their questions over the specified data source. | <details><summary>**See Components/Techniques**</summary> <ul><li><details><summary>Home Page</summary>This is the section where you build a RAG pipeline by instructing the "builder agent". Typically to setup a RAG pipeline you need the following components:<ul><li>Describe the dataset. Currently they support either a single local file or a web page</li><li>Describe the task. Concretely this description will be used to initialize the "system prompt" of the LLM powering the RAG pipeline.</li><li>Define the typical parameters for a RAG setup. See the below section for the list of parameters.</li></ul></details></li><li><details><summary>RAG Config</summary><ul><li>This section contains the RAG parameters, generated by the "builder agent" in the previous section. In this section, you have a UI showcasing the generated parameters and have full freedom to manually edit/change them as necessary.</li><li>Currently the set of parameters is as follows:</li><ul><li>System Prompt</li><li>Include Summarization: whether to also add a summarization tool (instead of only doing top-k retrieval.)</li><li>Top-K</li><li>Chunk Size</li><li>Embed Model</li><li>LLM</li></ul><li>If you manually change parameters, you can press the "Update Agent" button in order to update the agent.</li><li>If you don't see the `Update Agent` button, that's because you haven't created the agent yet. Please go to the previous "Home" page and complete the setup process.</li><li>We can always add more parameters to make this more "advanced" üõ†Ô∏è, but thought this would be a good place to start.</li></ul></details></li><li><details><summary>Generated RAG Agent</summary><ul><li>Once your RAG agent is created, you have access to this page.</li><li>This is a standard chatbot interface where you can query the RAG agent and it will answer questions over your data.</li><li>It will be able to pick the right RAG tools (either top-k vector search or optionally summarization) in order to fulfill the query.</li></ul></details></li><li><details><summary>Supported LLMs and Embeddings</summary><ul><li><details><summary>Builder Agent</summary><ul><li>By default, the builder agent uses OpenAI, specified in the <code>core/builder_config.py</code> file.</li><li>Customization to any desired LLM is possible, with an example provided for Anthropic.</li><li>Note: GPT-4 variants are recommended for reliable results in agent construction.</li></ul></details></li><li><details><summary>Generated RAG Agent</summary><ul><li>Configuration can be set through natural language or manually for both the embedding model and LLM.</li><li>LLMs supported:<ul><li>OpenAI: ID format is "openai:<model_name>", e.g., "openai:gpt-4-1106-preview".</li><li>Anthropic: ID format is "anthropic:<model_name>", e.g., "anthropic:claude-2".</li><li>Replicate: ID format is "replicate:<model_name>".</li><li>HuggingFace: ID format is "local:<model_name>", e.g., "local:BAAI/bge-small-en".</li></ul></li><li>Embeddings: Supports text-embedding-ada-002 by default, also supports Hugging Face models. Hugging Face models can be used by prefixing "local", e.g., "local:BAAI/bge-small-en".</li></ul></li></ul></details></li></ul></details> | <details><summary>**See Libraries**</summary><ul><li>streamlit</li><li>streamlit-pills</li><li>llama-index</li><li>llama-hub</li><li>langchain</li><li>pypdf</li><li>clip</li><li>typing-inspect</li><li>typing_extensions</li><li>types-requests</li><li>black</li><li>isort</li><li>pytest-asyncio</li><li>ruff</li><li>mypy</li><li>referencing</li><li>jsonschema-specifications</li><li>poetry</li><li>poetry-core</li></ul> | This project is very interesting since it gives us an idea on how we could implement the creation of RAG pipelines using natural language, which is very cool. This can lead into a very robust project for the creation of RAG pipelines since we can reach to people with no-code abilities.|
| [RAGArch](https://github.com/AI-ANK/RAGArch) | RAGArch is a streamlit application that allows users to test and compare RAG pipelines by giving them the option to choose different parameters such as the LLM, embeddings, and vector store. | <details><summary>**See Components/Techniques**</summary>The application not only allows users to play with different settings with their own documents in real time, but also provides the option to export the Python code that corresponds to the pipeline the users configures so that they can put it into another project. The demo utilizes streamlit and LlamaindexThe LLM's that are included in this demo are Gemini Pro, Cohere, GPT 3.5, and GPT 4. The embeddings included are: <ul><li>"BAAI/bge-small-en-v1.5"</li><li>"WhereIsAI/UAE-Large-V1"</li><li>"BAAI/bge-large-en-v1.5"</li><li>"khoa-klaytn/bge-small-en-v1.5-angle"</li><li>"BAAI/bge-base-en-v1.5"</li><li>"llmrails/ember-v1"</li><li>"jamesgpt1/sf_model_e5"</li><li>"thenlper/gte-large"</li><li>"infgrad/stella-base-en-v2"</li><li>"thenlper/gte-base"</li></lu> Finally for Vectors Stores there is Simple, Pinecone, and Qdrant</details> | <details><summary>**See Libraries**</summary><ul><li>google-generativeai</li><li>llama-index</li><li>openai</li><li>cohere</li><li>sentence_transformers</li><li>pypdf</li><li>transformers</li><li>tree_sitter</li><li>tree_sitter_languages</li><li>chromadb</li><li>qdrant-client</li><li>pinecone-client</li></ul></details> | This project aligns with the idea of creating the RAG pipelines using an interface where users don't need to code. This is useful to give us an idea what we should include and how we can do it. |

## Datasets

| Datasets | Overview | Size | Structure | To be used | Use | Notes | 
|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------|
| [EasyRag eval dataset for Wikipedia](https://huggingface.co/datasets/philschmid/easyrag-mini-wikipedia) | Dataset to evaluate RAG pipelines. It consists out of ~900 question and ground truth answers from Wikipedia articles. In addition to the questions it has a second config with documents for retrieval. The dataset can be used by "indexing" the documents and then using the question and ground truth to evaluate your pipeline.| ~1k rows | 2 subsets: <ul><li>Document: It offers the contexts for the questions and ground truths</li><li>Questions: It offers questions and their ground truths</li></ul> | ‚úÖ | Fine-tuning | Offers a reasonable size and nice structure. Will require to use a pipeline to create the answers manually. |
| [RAG Dataset 12000](https://huggingface.co/datasets/neural-bridge/rag-dataset-12000) | The Retrieval-Augmented Generation (RAG) Dataset 12000 is designed for RAG-optimized models, offering English data built by Neural Bridge AI and released under the Apache license 2.0. It enhances large language models (LLMs) by allowing them to consult external authoritative knowledge bases before generating responses. This approach significantly improves the models' ability to produce relevant, accurate, and context-specific output by extending their capabilities to specialized domains or internal data without retraining. | The dataset consists of triple-feature entries: "context," "question," and "answer" fields, with 12000 entries. | Each data point includes a context obtained from Falcon RefinedWeb, a question related to the context, and an answer generated by GPT-4. Data instances contain context, question, and answer fields. An example data point is structured as follows:<code>{"context": "...","question": "...","answer": "..."}</code> | ‚ùì | Fine-tuning | This dataset is useful because it gives a big dataset to fine-tune the pre-trained LLM. However, we will need to take a look at how to generate ground truthts since it is necessary to use RAGas framework to get the RAG pipelines metrics |
|[SQuAD 2.0](https://huggingface.co/datasets/rajpurkar/squad_v2)| Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. SQuAD 2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. | 142,192 rows | <ul><li>id: a string feature.</li><li>title: a string feature.</li><li>context: a string feature.</li><li>question: a string feature.</li><li>answers: a dictionary feature containing:<ul><li>text: a string feature.</li><li>answer_start: a int32 feature.</li></ul></li></ul> | ‚ùì | This dataset offers a huge amount of data in different contexts with a variatey of questions and answers. It doesn't offer a ground truth feature which might be a problem. |
| [HotPot QA](https://huggingface.co/datasets/hotpot_qa) | <details><summary>**See overview**</summary> HotpotQA is a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) it provides sentence-level supporting facts required for reasoning, allowingQA systems to reason with strong supervision and explain the predictions; (4) Offers a new type of factoid comparison questions to test QA systems‚Äô ability to extract relevant facts and perform necessary comparison.</details> | 105k rows | <ul><li>fullwiki</li><li>id: a string feature.</li><li>question: a string feature.</li><li>answer: a string feature.</li><li>type: a string feature.</li><li>level: a string feature.</li><li>supporting_facts: a dictionary feature containing:<ul><li>title: a string feature.</li><li>sent_id: a int32 feature.</li><li>context: a dictionary feature containing:<ul><li>title: a string feature.</li><li>sentences: a list of string features.</li></ul></li></ul></li></ul> | ‚ùì | Fine-tuning | This dataset is very good overall, presenting different context-domains with the necessary ground truths. The only problem is that this doesn't come directly from a RAG pipeline |
| [Dataset Card for RAG-Instruct-Benchmark-Tester](https://huggingface.co/datasets/llmware/rag_instruct_benchmark_tester) | <details><summary>**Overview**</summary>The updated benchmarking test dataset for Retrieval Augmented Generation (RAG) focuses on evaluating model performance in enterprise applications, particularly in financial services and legal domains. | It comprises 200 questions with context passages sourced from various common retrieval scenarios such as financial news, contracts, technical articles, etc. </details> | <details><summary>**Details**</summary>The dataset is segmented into several categories for benchmarking purposes: <ul><li>Core Q&A Evaluation (Samples 0-99): Consisting of 100 fact-based questions to assess the model's ability to provide correct responses, scored on a scale of 0-100.</li><li>Not Found Classification (Samples 100-119): Includes 20 samples where the context passage lacks a direct answer to the question, aiming to evaluate whether the model appropriately identifies such cases as "Not Found."</li><li>Boolean - Yes/No (Samples 120-139): Comprises 20 Yes/No questions to assess the model's handling of binary queries.</li><li>Basic Math (Samples 140-159): Features 20 everyday math questions covering simple arithmetic operations, percentages, sorting, and ranking.</li><li>Complex Q&A (Samples 160-179): Consists of 20 samples designed to test the model on various complex skills such as multiple-choice, financial table reading, and logical selections.</li><li>Summary (Sample 180-199): Includes 20 samples for evaluating the model's ability to generate both long-form and short-form summarizations.</li></ul></details> <details><summary>**Structure**</summary>200 JSONL samples with 6 keys: "query", "context" ,"answer", "category" ,"tokens", "sample_number"</details> | ‚ùå | Fine-tuning | This dataset is great since it offers broader domains, which is good for RAG evaluation. However the dataset is too small for be used to fine-tuning. | 
| [RAGas Evaluation](https://huggingface.co/datasets/shayorshay/ragas-eval) | <details><summary>**Overview**</summary>This dataset focus on providing data to use it with RAGAs framework for RAG pipeline evaluation</details> | <details><summary>**Details**</summary>This dataset has 30 rows, where each value is a string. It has four features that allows compatibility with RAGAs framework.</details> | <details><summary>**Structure**</summary>CSV rows samples with 4 keys: "question", "ground_truths" ,"answer", "context"</details> | ‚ùå | Fine-tuning | This dataset is good because it provides a good overview of how the dataset should look like, but the size again is not good enough. |
| [Amnesty QA](https://huggingface.co/datasets/explodinggradients/amnesty_qa) | <details><summary>**Overview**</summary>The dataset provides information for a RAG pipeline evaluation using the RAGAs framework. Ir provides the questions, answers, ground truths, and context. </details> | <details><summary>**Details**</summary>The dataset is avalaible in: English, Malayalam, Hindi. It consists of 20 rows. </details> <details> | <summary>**Structure**</summary><code>{"question": "...", "ground_truths": ["...",...] ,"answer": "...", "contexts": ["...",...]}</code></details> | ‚ùå | Fine-tuning | Like previous datasets, it offers an ideal structure but the size is not enough to fine-tune a LLM. |

## Techniques

| Technique | Overview | Pros | Cons | Use-cases | Tools/Software | To be used | Notes |
|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-------------|----------|
| [Synthetic Test Data generation](https://docs.ragas.io/en/stable/concepts/testset_generation.html) | Synthetic test data generation involves creating artificial data to evaluate the performance of systems or models. In the context of Ragas, this process focuses on generating QA samples from documents using evolutionary techniques. | 1. Reduces developer time in data aggregation process by up to 90%. 2. Enables the creation of diverse QA samples, covering various question types and difficulty levels. | 1. May not fully capture real-world scenarios or nuances. 2. Requires careful tuning to ensure generated data is representative and unbiased. | 1. Evaluating Retrieval-Augmented Generation (RAG) augmented pipelines. 2. Assessing the performance of language models in QA tasks. | Ragas, langchain, llama-index, OpenAI (for language models) | ‚úÖ | Technique is useful to compute the RAG pipeline performance metrics if the user doesn't define a set of questions and ground truths. |
| [Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA](https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07) | This technique involves fine-tuning a pre-trained large language model (LLM) on a custom dataset using QLoRA. | 1. Enhances model performance on domain-specific tasks by fine-tuning on relevant data. 2. Allows for customization of the language model to suit specific requirements or applications | 1. Requires a sufficient amount of high-quality labeled data for effective fine-tuning. 2. Fine-tuning process can be computationally expensive and time-consuming. 3. May require expertise in machine learning and natural language processing. | 1. Customizing language models for specific industries or domains. 2. Improving model accuracy on specialized tasks such as medical diagnosis or legal document analysis. | QLoRA (Questionable Logic Reasoning Assistant), large language models (e.g., GPT, BERT), machine learning frameworks (e.g., TensorFlow, PyTorch) | ‚ùì | Potential technique to fine-tune. Exploring all the options. |
| [Evaluation of RAG Pipelines for more reliable LLM applications](https://medium.com/@abhinavkimothi/evaluation-of-rag-pipelines-for-more-reliable-llm-applications-8037176ff30e#:~:text=By%20employing%20metrics%20such%20as,the%20effectiveness%20of%20RAG%20pipelines.) | This technique involves using the Ragas, a RAG pipeline evaluator framework, to achieve desired performances, understanding and using the performance metrics Ragas provides. | <ul><li>The framework is easy to use. Do not involve too much lines of code.</li><li>Offers a variety of metrics</li></ul> | <ul><li>For evaluation, the data requieres to be in a specific format with specific features (Asnwer, Ground Truth, Answer, Context).</li></ul> | <ul><li>Allows to evaluate a set of questions and answers from a RAG pipeline</li><li>Useful to understand how to label data for a specific performance goal.</li> | <ul><li>Ragas Framework</li><li>Langchain</li></ul> | ‚úÖ | Technique is important for pipeline evualtion and also to create our dataset for fine-tuning purposes. |
| [Multi-Label Classification Model From Scratch: Step-by-Step Tutorial](https://huggingface.co/blog/Valerii-Knowledgator/multi-label-classification) | This technique presents a way to achieve multi-label classification fine tuning a pre-trained LM for a custom dataset. | <ul><li>Leveraging pre-trained LMs allows for capturing rich linguistic patterns and semantics, which can lead to better performance, especially with limited labeled data.</li><li>The approach provides flexibility in terms of choosing pre-trained models and adapting them to specific multi-label classification tasks.</li></ul> | <ul><li>Fine-tuning large pre-trained LMs can be computationally intensive, requiring significant GPU resources and time.</li><li>While fine-tuning can mitigate the need for extensive labeled data, sufficient labeled examples are still required to adapt the model to the target task effectively.</li></ul> | <ul><li>Classifying documents or articles into multiple categories simultaneously, such as topic classification, sentiment analysis, or content tagging.</li><li>Can be applied to perform multi-classification on what needs to be improved for RAG pipelines.</li></ul> | <ul><li>Transformers</li><li>HuggingFace</li></ul> |‚ùì | This technique gives a good approach to achieve our goal of having a fine-tuned LLM capable of classify the RAG pipeline using the metrics. In this case it will be classified using multi-classes. |
| [**Visualize your RAG Data ‚Äî Evaluate your Retrieval-Augmented Generation System with Ragas**](https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557) | The technique outlined in the article introduces a methodology for evaluating Retrieval-Augmented Generation (RAG) systems through interactive visualization techniques. By analyzing metrics like answer correctness alongside question and document embeddings using UMAP-based visualizations, the approach offers a comprehensive means to assess and understand RAG system performance, aiding informed decision-making in software development. | <ul><li>The approach supports informed decision-making in software development by offering insights into RAG system behavior, allowing developers to identify areas for improvement and optimize system performance.</li></ul> | <ul><li>Utilizing interactive visualization techniques and interpreting results may require a certain level of technical expertise, potentially limiting accessibility to those with data analysis skills.</li></ul> | Developers can apply the approach to assess the performance of RAG models integrated into software applications, such as virtual assistants, chatbots, and information retrieval systems. | <ul><li>Ragas Framework</li><li>Langchain</li><li>Renumics-Spotlight</li></ul> |‚ùì| Possible technique for the interface. The user can explore and get their own insights by looking at the visualizations. |
| [**Top Evaluation Metrics for RAG Failures**](https://towardsdatascience.com/top-evaluation-metrics-for-rag-failures-acb27d2a5485) | The article outlines a comprehensive approach to evaluating Large Language Model (LLM) Retrieval-Augmented Generation (RAG) systems, emphasizing continuous assessment against established metrics to improve accuracy, relevance, and timeliness in information provision. It also discusses advanced techniques such as re-ranking, metadata attachments, and experimentation with embedding models and indexing methods, which, although resource-intensive, can enhance contextual coherence and overall performance when used alongside RAG, contingent upon effective monitoring of retrieval and response metrics. |<ul><li>Comprehensive Evaluation Metrics: The article advocates for the use of comprehensive evaluation metrics for assessing Large Language Model (LLM) Retrieval-Augmented Generation (RAG) systems, which can lead to improved accuracy, relevance, and timeliness in information provision.</li><li>Continuous Improvement: Emphasizing the importance of continuous evaluation, the article promotes the iterative enhancement of RAG systems over time, ensuring that they remain effective and up-to-date in meeting user needs.</li></ul> | <ul><li>Resource Intensive: Advanced improvement techniques such as re-ranking and experimenting with embedding models and indexing methods may require substantial computational resources, which could be a limiting factor for some organizations with resource constraints.</li><li>Complex Implementation: Implementing advanced techniques like HyDE and Cohere document mode may require specialized expertise and careful configuration, increasing the complexity of system development and maintenance.</li></ul> | <ul><li>Chatbots and Virtual Assistants: RAG systems enhanced with advanced techniques can be deployed in chatbots and virtual assistants to provide more contextually coherent and accurate responses to user queries, improving user experience and engagement.</li><li>Educational Technology: Educational technology providers can leverage RAG systems to create personalized learning experiences for students, delivering relevant and informative content based on individual learning goals and preferences.</li></ul>| | ‚ùì | Possible educational resource to improve RAG performance alongside the fine-tuned LLM asnwers. |
|[Fine-tuning BERT for Text classification](https://www.kaggle.com/code/neerajmohan/fine-tuning-bert-for-text-classification)| This technique is to achieve text classification using the power of a LLM. Transfer learning technique is used to perform the text classification problem. | <ul><li>Time Efficiency: Fine-tuning pretrained BERT models saves time as they already encode a lot of information, reducing the need for extensive training on large datasets.</li><li>Data Efficiency: Pretrained BERT models perform well even with small datasets, leveraging the knowledge gained from being trained on large text corpora.</li></ul>|<ul><li>Complexity: Fine-tuning BERT models may require technical expertise to implement and optimize, especially when dealing with specific task requirements or model configurations.</li><li>Resource Intensive: Fine-tuning BERT models can be computationally expensive, requiring sufficient computational resources such as GPU accelerators for efficient training.</li></ul> | <ul><li>Natural Language Processing (NLP) Tasks: BERT fine-tuning can be applied to various NLP tasks such as sentiment analysis, named entity recognition, text classification, and question answering.</li><li>Social Media Analysis: Predicting whether tweets or social media posts are about real disasters or not is just one example of how BERT fine-tuning can be used for social media analysis tasks.</li><li>Customer Support Automation: Implementing BERT-based models for text classification can enhance customer support automation systems by accurately categorizing customer queries and routing them to appropriate support channels.</li></ul>| <ul><li>Transformers</li><li>Pytorch</li><li>BERT</li></ul> | ‚ùì | Possible way to achieve the desired goal of the fine-tuned LLM capable of classify the RAG pipelines |
|[Building an AI Chatbot with RAG, Langchain, and Streamlit](https://www.linkedin.com/pulse/building-ai-chatbot-rag-langchain-streamlit-sachin-samuel-n9tef/)| This technique presents a way to integrate a robust LLM framework like Langchain, and a friendly front-end framework like Streamlit to showcase RAG capabilities. | <ul><li>Integration of Robust Framework: The technique integrates Langchain, a robust Large Language Model (LLM) framework, providing powerful language processing capabilities.</li><li>User-Friendly Front-End: Streamlit, a friendly front-end framework, is used to create an intuitive and interactive interface for showcasing RAG capabilities.</li></ul> | <ul><li>Technical Complexity: Implementing an AI chatbot with RAG, Langchain, and Streamlit may require a certain level of technical expertise, especially in setting up and configuring the components.</li><li>Resource Intensive: Running an AI chatbot with these frameworks may be resource-intensive, requiring sufficient computational resources and infrastructure.</li></ul>| <ul>  <li>Customer Service: The AI chatbot can be deployed for customer service applications, providing automated responses to customer inquiries and support requests.</li><li>Information Retrieval: Users can interact with the chatbot to retrieve information on various topics, leveraging the language processing capabilities of Langchain and the interactive interface of Streamlit.</li><li>Educational Tools: The chatbot can serve as an educational tool, assisting users with learning and understanding concepts in natural language processing and AI.</li></ul> | <ul><li>Streamlit</li><li>Langchain</li></ul> | ‚úÖ | Optimum technique to showcase our project to the world. Streamlit is easy to use, and langchain a very powerful framework. |


## More About Us
[Know about our team](https://docs.google.com/presentation/d/1SBlGVdz81NUZDpsXQ5xZXaC7oOi-OAkKURFXmy4CcT8/edit?usp=sharing)
